{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zZxq2zP3Se74",
        "7T8U5k5yUP-0",
        "in2EinpnXMhg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzu0-HHDRBxV"
      },
      "source": [
        "# 1.Import Common Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "\n",
        "tensorflow.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "v6v3eW72iQIn",
        "outputId": "b9b66a56-497d-440c-cb4d-a6e5b0262707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.13.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow.keras.optimizers.SGD()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZhxLQsgiYfa",
        "outputId": "c89d7ca3-1956-4b76-91a1-64bfd4bc1cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.optimizers.sgd.SGD at 0x7e09e40a23b0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc8ZEyY5RFx7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from jcopml.pipeline import num_pipe, cat_pipe\n",
        "from jcopml.utils import save_model, load_model\n",
        "from jcopml.plot import plot_missing_value\n",
        "from jcopml.feature_importance import mean_score_decrease"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZxq2zP3Se74"
      },
      "source": [
        "#2.Import CSV DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QQngcf-Sk4p"
      },
      "source": [
        "df = pd.read_csv(\"____________\", index_col=\"___________\", parse_dates=[\"____________\"])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912-3nEHSs8O"
      },
      "source": [
        "#3.Datasets Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iepcZ-NS3Et"
      },
      "source": [
        " stratified shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mukSAWgJS54o"
      },
      "source": [
        "## stratified shuffle\n",
        "X = df.drop(columns=\"___________\")\n",
        "y = \"_____________\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2xkyKKGTMLm"
      },
      "source": [
        "Shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgVN7fOjTTYf"
      },
      "source": [
        "## Shuffle\n",
        "X = df.drop(columns=\"___________\")\n",
        "y = \"_____________\"\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NpcgZqmTcuh"
      },
      "source": [
        "#4.Preprocessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o90GVlHxUJon"
      },
      "source": [
        "## common"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyh_pal8UIKW"
      },
      "source": [
        "#COMMON\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('numeric', num_pipe(), [\"______________\"]),\n",
        "    ('categoric', cat_pipe(encoder='onehot'), [\"_____________\"]),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T8U5k5yUP-0"
      },
      "source": [
        "## Advanced Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkokEdqjUVMS"
      },
      "source": [
        "# ADVANCED\n",
        "# Note: You could not use gsp, rsp, and bsp recommendation in advance mode\n",
        "# You should specify your own parameter grid / interval when tuning\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('numeric1', num_pipe(impute='mean', poly=2, scaling='standard', transform='yeo-johnson'), [\"______________\"]),\n",
        "    ('numeric2', num_pipe(impute='median', poly=2, scaling='robust'), [\"______________\"]),\n",
        "    ('categoric1', cat_pipe(encoder='ordinal'), [\"_____________\"]),\n",
        "    ('categoric2', cat_pipe(encoder='onehot'), [\"_____________\"])\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1CCTuNdWK__"
      },
      "source": [
        "#5.Supervised Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39fqOzwyWSnP"
      },
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', KNeighborsRegressor())\n",
        "])\n",
        "\n",
        "#SVM\n",
        "from sklearn.svm import SVR\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', SVR(max_iter=500))\n",
        "])\n",
        "\n",
        "\n",
        "#RF\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', RandomForestRegressor(n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "#XGBOOST\n",
        "from xgboost import XGBRegressor\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', XGBRegressor(n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "#LINEAR REGRESSION\n",
        "from sklearn.linear_model import LinearRegression\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', LinearRegression())\n",
        "])\n",
        "\n",
        "\n",
        "#ELASTIC NET\n",
        "from sklearn.linear_model import ElasticNet\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', ElasticNet())\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in2EinpnXMhg"
      },
      "source": [
        "#5.Supervised Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVGC0_VQXRgR"
      },
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "\n",
        "#SVM\n",
        "from sklearn.svm import SVC\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', SVC(max_iter=500))\n",
        "])\n",
        "\n",
        "\n",
        "#RF\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', RandomForestClassifier(n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "#XGBOOST\n",
        "from xgboost import XGBClassifier\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', XGBClassifier(n_jobs=-1, random_state=42))\n",
        "])\n",
        "\n",
        "\n",
        "#LOGISTIC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', LogisticRegression(solver='lbfgs', n_jobs=-1, random_state=42))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2DmqH0KYRS3"
      },
      "source": [
        "#6.TUNING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skopt.space import Real, Categorical, Integer\n",
        "params = {\n",
        "'algo__max_depth': Integer(low=-1, high=12, transform='identity'),\n",
        "'algo__learning_rate': Real(low=0.01, high=1, prior='log-uniform', transform='identity'),\n",
        "'algo__colsample_bytree': Real(low=0.1, high=1, transform='identity'),\n",
        "'algo__subsample': Real(low=0.2, high=0.8, transform='identity'),\n",
        "'algo__num_leaves': Integer(low=20, high=100, transform='identity'),\n",
        "'algo__reg_alpha': Real(low=0.001, high=10, prior='log-uniform', transform='identity'),\n",
        "'algo__reg_lambda': Real(low=0.001, high=10, prior='log-uniform', transform='identity')\n",
        "}"
      ],
      "metadata": {
        "id": "9xUyNIGTJUE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOee8VddYU3c"
      },
      "source": [
        "#GRID SEARCH CV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from jcopml.tuning import grid_search_params as gsp\n",
        "\n",
        "model = GridSearchCV(pipeline, gsp.\"_______________\", cv=\"___\", scoring='___', n_jobs=-1, verbose=1)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S5OcLTQYaRD"
      },
      "source": [
        "#RANDOMIZED SEARCH CV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from jcopml.tuning import random_search_params as rsp\n",
        "\n",
        "model = RandomizedSearchCV(pipeline, rsp.\"_______________\", cv=\"___\", scoring='___', n_iter=\"___\", n_jobs=-1, verbose=1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZrhtZvxYf7U"
      },
      "source": [
        "#BAYESIAN\n",
        "from skopt import BayesSearchCV\n",
        "from jcopml.tuning import bayes_search_params as bsp\n",
        "\n",
        "model = BayesSearchCV(pipeline, bsp.\"_______________\", cv=\"___\", scoring=\"__\", n_iter=\"___\", n_jobs=-1, verbose=1, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.score(X_train, y_train), model.best_score_, model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Halving Search\n",
        "\n",
        "from sklearn.experimental import enable_halving_search_cv\n",
        "from sklearn.model_selection import HalvingRandomSearchCV\n",
        "from jcopml.tuning import random_search_params as rsp\n",
        "\n",
        "model_hs = HalvingRandomSearchCV(pipeline,_____,cv = 3,n_candidates = 300,scoring=\"balanced_accuracy\",min_resources =\"exhaust\",n_jobs = -1,verbose = -1,factor=2 )\n",
        "model_hs.fit(X_train,y_train)\n",
        "\n",
        "print(model.best_params_)\n",
        "print(model.score(X_train, y_train),model.best_score_, model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "2o-pFAaDa3ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0hgMRvpYpbU"
      },
      "source": [
        "#7.SAVE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ0LGTyrYyai"
      },
      "source": [
        "save_model(model, \"__________.pkl\")\n",
        "\n",
        "save_model(model.best_estimator_, \"__________.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spot check Algorithm"
      ],
      "metadata": {
        "id": "R-HhvgqhyW4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_model = pd.DataFrame(columns = [\"model\",\"set_data\",\"score\"])\n",
        "set_data = [\"test\",\"train\"]\n",
        "models = {\n",
        "        \"KNN\" : KNeighborsClassifier(),\n",
        "        \"SVM\":SVC(),\n",
        "        \"Random Forest\":RandomForestClassifier(random_state = 42,n_jobs = -1),\n",
        "        \"Logistic Regression\" : LogisticRegression(random_state = 42),\n",
        "        \"LGBM\" : LGBMClassifier(random_state = 42),\n",
        "        \"XGB\" : XGBClassifier(random_state = 42)\n",
        "        }\n",
        "\n",
        "scorer = \"accuracy\"\n",
        "num_cv = 5\n",
        "cv = StratifiedKFold(n_splits = num_cv,shuffle = True,random_state = 42)\n",
        "\n",
        "for m in models:\n",
        "    pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', models[m])\n",
        "])\n",
        "    spot_check = cross_val_score(pipeline,X_train,y_train,cv = cv,scoring = scorer,n_jobs= -1 )\n",
        "    spot_check = spot_check.mean()\n",
        "    model = pipeline.fit(X_train,y_train)\n",
        "    score = pipeline.score(X_test,y_test)\n",
        "    model_list = [m] * 2\n",
        "    tes = pd.DataFrame(list(zip(model_list,set_data,[score,spot_check])),columns = [\"model\",\"set_data\",\"score\"])\n",
        "    df_model = pd.concat([df_model,tes],ignore_index = True)"
      ],
      "metadata": {
        "id": "t_GCtxXjyaqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set figsize\n",
        "plt.figure(figsize=(10, 5))\n",
        "plots = sns.barplot(x=\"model\", y=\"score\", data=df_model, ci=None,hue = \"set_data\")\n",
        "plots.set_title(\"Mean Score of Model\")\n",
        "plots.bar_label(plots.containers[0],fmt = \"%.3f\")\n",
        "plots.bar_label(plots.containers[1],fmt = \"%.3f\")\n",
        "plt.yticks(np.arange(0,1.1,step = 0.1))\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "JL32KxgK3Hez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "models = {\"KNN\":KNeighborsClassifier(),\n",
        "        \"SVM\":SVC(),\n",
        "        \"RF\":RandomForestClassifier(random_state = 42,n_jobs = -1),\n",
        "        \"XGB\":XGBClassifier(n_jobs=-1, random_state=42),\n",
        "        \"LGBM\":LGBMClassifier(random_state=42,n_jobs=-1)}\n",
        "\n",
        "scorer = \"accuracy\"\n",
        "for model in models:\n",
        "    pipeline = Pipeline([\n",
        "    ('prep', preprocessor),\n",
        "    ('algo', models[model])\n",
        "])\n",
        "    spot_check = cross_val_score(pipeline,X_train,y_train,cv = 5,scoring = scorer,n_jobs= -1 )\n",
        "    print(model + \":\" + str(round(spot_check.mean(),3)) + \"(\" + str(round(spot_check.std(),3)) +\")\")"
      ],
      "metadata": {
        "id": "PXJgS2Yl0SNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n",
        "\n"
      ],
      "metadata": {
        "id": "rekNvWcELTY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Value"
      ],
      "metadata": {
        "id": "22lAxqKS2yG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.displot(\n",
        "    data=___.isna().melt(value_name=\"missing\"),\n",
        "    y=\"variable\",\n",
        "    hue=\"missing\",\n",
        "    multiple=\"fill\",\n",
        "    aspect=2\n",
        ")\n",
        "plt.title(\"Missing Value Proportion Each Feature\");"
      ],
      "metadata": {
        "id": "iAkNCS3XSrUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Matrix"
      ],
      "metadata": {
        "id": "OJy8Ia19iGXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.corr(method=\"spearman\"),cmap = \"bwr\" ,vmin = -1,vmax=1,annot = True,cbar = False,fmt = \".2f\")"
      ],
      "metadata": {
        "id": "84UPXLzqiKzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Numerical Data\n"
      ],
      "metadata": {
        "id": "6QvlZxnqLeJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot numerical data (Classification Task)\n",
        "# Plot KDE 1 Tiap Feature\n",
        "numerical = df_set1.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "fig, axes = plt.subplots(7,8,figsize = (20,10))\n",
        "fig.subplots_adjust(hspace=1, wspace=0.5)\n",
        "row = 0\n",
        "cols = 0\n",
        "\n",
        "for item in numerical:\n",
        "    if cols > 7:\n",
        "        cols = 0\n",
        "        row += 1\n",
        "    ax = sns.histplot(x=item,data=df_set1 ,ax=axes[row, cols],bins = 20)\n",
        "    ax.set_ylabel(\"\")\n",
        "    cols += 1\n"
      ],
      "metadata": {
        "id": "4ZtNBIIHLprg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot KDE Berdasarkan Target\n",
        "def plot_kde(data,start,end,row_num,col_num,title,target ):\n",
        "    df_set_pos = data[data[target] == ____].drop(target,axis = 1)\n",
        "    df_set_neg = data[data[target] == _____].drop(target,axis = 1)\n",
        "    df_set_pos = df_set_pos.iloc[:,start:end+1]\n",
        "    df_set_neg = df_set_neg.iloc[:,start:end +1]\n",
        "    #plot numerical data (Classification Task)\n",
        "    numerical = df_set_pos.select_dtypes(include=['int64', 'float64']).columns\n",
        "    fig, axes = plt.subplots(row_num,col_num,figsize = (10,8))\n",
        "    fig.subplots_adjust(hspace=0.7, wspace=0.3)\n",
        "    fig.suptitle(title,fontsize = 20)\n",
        "    row = 0\n",
        "    cols = 0\n",
        "\n",
        "    for item in numerical:\n",
        "        if cols > col_num -1:\n",
        "            cols = 0\n",
        "            row += 1\n",
        "        ax1 = sns.kdeplot(x = item,data = df_set_pos,ax = axes[row,cols],color = \"____\" ,label = \"____\")\n",
        "        ax2 = sns.kdeplot(x = item,data = df_set_neg,ax = axes[row,cols],color = \"_____\" ,label = \"____\")\n",
        "        ax1.legend()\n",
        "        ax1.set_ylabel(\"\")\n",
        "        ax2.set_ylabel(\"\")\n",
        "        cols += 1\n",
        "\n",
        "plot_kde(df,0,4,3,2,'Distribusi IPS berdasarkan Keterlambatan',\"TERLAMBAT\")"
      ],
      "metadata": {
        "id": "zlbJx6SvYPkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot Histogram,KDE,Boxplot\n",
        "def num_eda(data,feature,target,bins ):\n",
        "    df_set_pos = data[data[target] == ____].drop(target,axis = 1)\n",
        "    df_set_neg = data[data[target] == ____].drop(target,axis = 1)\n",
        "    #plot numerical data (Classification Task)\n",
        "    fig, axes = plt.subplots(2,2,figsize = (18,4))\n",
        "\n",
        "    ax1 = sns.histplot(x = feature,data = data,ax = axes[0,0],bins = bins,kde = True,edgecolor = \"k\",color = \"orange\")\n",
        "    ax1.grid(linestyle='--', linewidth=0.5, color='gray')\n",
        "    ax1.set_title(f\"{feature} Distribution\")\n",
        "\n",
        "    ax2 = sns.histplot(x = feature,data = df_set_pos,ax = axes[0,1],bins = bins,label =\"____\",kde = True,color = \"green\",linewidth = 0 )\n",
        "    ax2_1 = sns.histplot(x = feature,data = df_set_neg,ax = axes[0,1],label = \"Not - ______\",bins = bins,kde = True,color = \"red\",linewidth = 0)\n",
        "    ax2.grid(linestyle='--', linewidth=0.1, color='gray')\n",
        "    ax2.set_title(f\"{feature} Distribution by Target Class\")\n",
        "    ax2.legend()\n",
        "\n",
        "    ax3 = sns.boxplot(x = feature,data = data,ax = axes[1,0],color = \"orange\")\n",
        "    ax3.grid(linestyle='--', linewidth=0.5, color='gray')\n",
        "\n",
        "    ax4 = sns.boxplot(x = feature,y = target,data = data,ax = axes[1,1],orient = \"h\",palette = [\"red\",\"green\"])\n",
        "    ax4.grid(linestyle='--', linewidth=0.1, color='gray')\n",
        "    ax4.legend()"
      ],
      "metadata": {
        "id": "JWRrvPOntHiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot Categorical Data"
      ],
      "metadata": {
        "id": "3NwP3IN7MIA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot categorical Data (classification Task)\n",
        "categorical = df.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(20, 10))\n",
        "fig.suptitle('Categorical Data')\n",
        "row = 0\n",
        "cols = 0\n",
        "for item in categorical:\n",
        "    if (row == 0 & cols == 0) :\n",
        "        sns.countplot(y=item, hue=\"y\", data=df, ax=axes[row][cols]).set_xlabel(\"\")\n",
        "\n",
        "    else:\n",
        "        sns.countplot(x=item, hue=\"y\", data=df, ax=axes[row][cols]).set_ylabel(\"\")\n",
        "    cols += 1\n",
        "    if cols == 4:\n",
        "        cols = 0\n",
        "        row += 1"
      ],
      "metadata": {
        "id": "yDjUJuHFMMoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot categorical Data (classification Task)\n",
        "def cat_eda(data,feature,target):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1 = sns.countplot(x=feature, data=data, ax=axes[0],color = \"orange\",edgecolor = \"k\")\n",
        "    ax1.bar_label(ax1.containers[0])\n",
        "    ax1.set_title(f\"{feature} Distribution\")\n",
        "\n",
        "    ax2 = sns.countplot(x = feature,hue = target, data = data,ax = axes[1],palette = [\"r\",'lime'],edgecolor = \"k\")\n",
        "    ax2.set_title(f\"{feature} Distribution with target\")\n",
        "    ax2.bar_label(ax2.containers[0])\n",
        "    ax2.bar_label(ax2.containers[1])"
      ],
      "metadata": {
        "id": "RSj1FO4YJ9nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for training\n",
        "def train_model_ts(list_model,X_train,y_train,X_test,y_test,metric,cv,scorer,pipeline,groups):\n",
        "    df_model = pd.DataFrame(columns = [\"model_name\",\"set_data\",\"score\",\"model\"])\n",
        "    set_data = [\"test\",\"cv\",\"train\"]\n",
        "\n",
        "    for m in list_model:\n",
        "        pipeline_copy = deepcopy(pipeline)\n",
        "        pipeline_copy.set_params(model = list_model[m])\n",
        "        spot_check = cross_val_score(pipeline_copy,X_train,y_train,cv = cv,scoring = scorer,n_jobs= -1,groups = groups )\n",
        "        spot_check = spot_check.mean()\n",
        "        model = pipeline_copy.fit(X_train,y_train)\n",
        "        score = metric(y_test,model.predict(X_test),squared = False)\n",
        "        score_train = metric(y_train,model.predict(X_train),squared = False)\n",
        "        model_list = [m] * 3\n",
        "        tes = pd.DataFrame(list(zip(model_list,set_data,[score,spot_check,score_train],[model,model,model])),columns = [\"model_name\",\"set_data\",\"score\",\"model\"])\n",
        "        print(f\"model {m} selesai di training\")\n",
        "        print(f\"score test {score}\")\n",
        "        print(f\"score cv {spot_check}\")\n",
        "        print(f\"score train {score_train}\")\n",
        "        print(\"=====================================\")\n",
        "        df_model = pd.concat([df_model,tes],ignore_index = True)\n",
        "\n",
        "    return df_model\n",
        "\n",
        "\n",
        "#function for feature selection\n",
        "#since im using pipeline for my workflow, so ineed to modify the rfecv function from scikit-learn,\n",
        "# so it can work with pipeline. other than that is same as scikit-learn rfecv function\n",
        "def rfecv(X, y, pipeline,min_features_to_select=3, cv = 3,step=3,scoring_metric=\"f1\",scoring_decimals=3,random_state=42,groups = None):\n",
        "    # Initialize survivors and ranked list\n",
        "    estimator = deepcopy(pipeline)\n",
        "    estimator.steps.pop(-1)\n",
        "    survivors = estimator.fit_transform(X_train,y_train).columns.tolist()\n",
        "    ranks = []\n",
        "    scores = []\n",
        "    # While the survivor list is longer than min_features_to_select\n",
        "    while len(survivors) >= min_features_to_select:\n",
        "        print(ranks)\n",
        "        remove_column_transformer = FunctionTransformer(lambda x: x.drop(ranks, axis=1))\n",
        "        estimator = deepcopy(pipeline)\n",
        "        estimator.steps.insert(-1, ('remove_column_transformer', remove_column_transformer))\n",
        "        # Get only the surviving features\n",
        "\n",
        "        # Train and get the scores, cross_validate clones\n",
        "        # the model internally, so this does not modify\n",
        "        # the estimator passed to this function\n",
        "        print(\"[%.2f] evaluating %i features ...\" % (time(), len(survivors)))\n",
        "        cv_result = cross_validate(estimator, X, y,\n",
        "                                cv=cv,\n",
        "                                groups = groups,\n",
        "                                scoring=scoring_metric,\n",
        "                                return_estimator=True)\n",
        "        # Append the mean performance to\n",
        "        score = np.mean(cv_result[\"test_score\"])\n",
        "        if scoring_decimals is None:\n",
        "            scores.append(score)\n",
        "        else:\n",
        "            scores.append(round(score, scoring_decimals))\n",
        "        print(\"[%.2f] ... score %f.\" % (time(), scores[-1]))\n",
        "\n",
        "        # Get feature weights from the model fitted\n",
        "        # on the best fold and square the weights as described\n",
        "        # in the paper. If the estimator is a Pipeline,\n",
        "        # we get the weights from the last element.\n",
        "        best_estimator = cv_result[\"estimator\"][np.argmax(cv_result[\"test_score\"])]\n",
        "        if isinstance(best_estimator, Pipeline):\n",
        "            weights = best_estimator[-1].feature_importances_\n",
        "        else:\n",
        "            weights = best_estimator.feature_importances_\n",
        "        weights = list(np.power(weights, 2))\n",
        "\n",
        "        # Remove step features (but respect min_features_to_select)\n",
        "        for _ in range(max(min(step, len(survivors) - min_features_to_select), 1)):\n",
        "\n",
        "            # Find the feature with the smallest ranking criterion\n",
        "            # and update the ranks and survivors\n",
        "            idx = np.argmin(weights)\n",
        "            ranks.insert(0, survivors.pop(idx))\n",
        "            weights.pop(idx)\n",
        "\n",
        "    # Calculate the best set of surviving features\n",
        "    ranks_reverse = list(reversed(ranks))\n",
        "    last_max_idx = len(scores) - np.argmax(list(reversed(scores))) - 1\n",
        "    removed_features = set(ranks_reverse[0:last_max_idx * step])\n",
        "    best_features = [f for f in X.columns if f not in removed_features]\n",
        "\n",
        "    # Return ranks and scores\n",
        "    return best_features, max(scores), ranks, scores"
      ],
      "metadata": {
        "id": "siq3yt8oTiyK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}