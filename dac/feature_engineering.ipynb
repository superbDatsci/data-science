{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold,RepeatedStratifiedKFold, cross_validate\n",
    "from copy import deepcopy\n",
    "\n",
    "from time import time\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(list_model,X_train,y_train,X_test,y_test,metric,cv,scorer,pipeline):\n",
    "    df_model = pd.DataFrame(columns = [\"model_name\",\"set_data\",\"score\",\"model\"])\n",
    "    set_data = [\"test\",\"cv\",\"train\"]\n",
    "\n",
    "    for m in list_model: \n",
    "        pipeline_copy = deepcopy(pipeline)\n",
    "        pipeline_copy.set_params(model = list_model[m])\n",
    "        spot_check = cross_val_score(pipeline_copy,X_train,y_train,cv = cv,scoring = scorer,n_jobs= -1)\n",
    "        spot_check_acc = cross_val_score(pipeline_copy,X_train,y_train,cv = cv,scoring = \"accuracy\",n_jobs= -1)\n",
    "        spot_check = spot_check.mean()\n",
    "        spot_check_acc = spot_check_acc.mean()\n",
    "        model = pipeline_copy.fit(X_train,y_train)\n",
    "        score = metric(y_test,model.predict(X_test),average = \"macro\")\n",
    "        score_train = metric(y_train,model.predict(X_train),average = \"macro\")\n",
    "        acc_score = model.score(X_test,y_test)\n",
    "        model_list = [m] * 3\n",
    "        tes = pd.DataFrame(list(zip(model_list,set_data,[score,spot_check,score_train],[model,model,model])),columns = [\"model_name\",\"set_data\",\"score\",\"model\"])\n",
    "        print(f\"model {m} selesai di training\")\n",
    "        print(f\"score test {score}\")\n",
    "        print(f\"score cv {spot_check}\")\n",
    "        print(f\"score train {score_train}\")\n",
    "        print(f\"acc score test {acc_score}\")\n",
    "        print(f\"acc score cv {spot_check_acc}\")\n",
    "        print(\"=====================================\")\n",
    "        df_model = pd.concat([df_model,tes],ignore_index = True)\n",
    "        \n",
    "    return df_model\n",
    "\n",
    "def rfecv(X, y, pipeline,min_features_to_select=3, cv = 3,step=3,scoring_metric=\"f1\",scoring_decimals=3,random_state=42,groups = None):\n",
    "    # Initialize survivors and ranked list\n",
    "    estimator = deepcopy(pipeline)\n",
    "    estimator.steps.pop(-1)\n",
    "    survivors = estimator.fit_transform(X,y).columns.tolist()\n",
    "    ranks = []\n",
    "    scores = []\n",
    "    while len(survivors) >= min_features_to_select:\n",
    "        remove_column_transformer = FunctionTransformer(lambda x: x.drop(ranks, axis=1))\n",
    "        estimator = deepcopy(pipeline)\n",
    "        estimator.steps.insert(-1, ('remove_column_transformer', remove_column_transformer))\n",
    "        print(\"[%.2f] evaluating %i features ...\" % (time(), len(survivors)))\n",
    "        cv_result = cross_validate(estimator, X, y,\n",
    "                                cv=cv,\n",
    "                                groups = groups,\n",
    "                                scoring=scoring_metric,\n",
    "                                return_estimator=True)\n",
    "        score = np.mean(cv_result[\"test_score\"])\n",
    "        if scoring_decimals is None:\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            scores.append(round(score, scoring_decimals))            \n",
    "        print(\"[%.2f] ... score %f.\" % (time(), scores[-1]))\n",
    "        \n",
    "        best_estimator = cv_result[\"estimator\"][np.argmax(cv_result[\"test_score\"])]\n",
    "        if isinstance(best_estimator, Pipeline):\n",
    "            weights = best_estimator[-1].feature_importances_\n",
    "        else:\n",
    "            weights = best_estimator.feature_importances_\n",
    "        weights = list(np.power(weights, 2))\n",
    "        for _ in range(max(min(step, len(survivors) - min_features_to_select), 1)):\n",
    "            idx = np.argmin(weights)\n",
    "            ranks.insert(0, survivors.pop(idx))\n",
    "            weights.pop(idx)\n",
    "    ranks_reverse = list(reversed(ranks))\n",
    "    last_max_idx = len(scores) - np.argmax(list(reversed(scores))) - 1\n",
    "    removed_features = set(ranks_reverse[0:last_max_idx * step])\n",
    "    best_features = [f for f in X.columns if f not in removed_features]\n",
    "    return best_features, max(scores), ranks, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_diff_srv_rate</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>dst_host_srv_diff_host_rate</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>dst_host_srv_serror_rate</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "      <th>dst_host_srv_rerror_rate</th>\n",
       "      <th>type_of_attack</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>SH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>private</td>\n",
       "      <td>S0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>neptune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>285.0</td>\n",
       "      <td>3623.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>228.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>232.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>255.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>smtp</td>\n",
       "      <td>SF</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>327.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>154.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type  service flag  src_bytes  dst_bytes  land  \\\n",
       "0       0.0           tcp  private   SH        0.0        0.0   0.0   \n",
       "1       0.0           tcp  private   S0        0.0        0.0   0.0   \n",
       "2       0.0           tcp     http   SF      285.0     3623.0   0.0   \n",
       "3       0.0           tcp     http   SF      232.0      584.0   0.0   \n",
       "4       1.0           tcp     smtp   SF     1080.0      327.0   0.0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
       "0             0.0     0.0  0.0  ...                 1.0   \n",
       "1             0.0     0.0  0.0  ...                 5.0   \n",
       "2             0.0     0.0  0.0  ...               228.0   \n",
       "3             0.0     0.0  0.0  ...               255.0   \n",
       "4             0.0     0.0  0.0  ...               154.0   \n",
       "\n",
       "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
       "0                    0.01                    0.94   \n",
       "1                    0.02                    0.08   \n",
       "2                    1.00                    0.00   \n",
       "3                    1.00                    0.00   \n",
       "4                    0.58                    0.02   \n",
       "\n",
       "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
       "0                         0.95                          0.0   \n",
       "1                         0.00                          0.0   \n",
       "2                         0.01                          NaN   \n",
       "3                         0.17                          NaN   \n",
       "4                         0.00                          NaN   \n",
       "\n",
       "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
       "0                  0.95                       1.0                   0.0   \n",
       "1                  1.00                       1.0                   0.0   \n",
       "2                  0.00                       0.0                   0.0   \n",
       "3                  0.00                       0.0                   0.0   \n",
       "4                  0.00                       0.0                   0.0   \n",
       "\n",
       "   dst_host_srv_rerror_rate  type_of_attack  \n",
       "0                       0.0            nmap  \n",
       "1                       0.0         neptune  \n",
       "2                       0.0          normal  \n",
       "3                       0.0          normal  \n",
       "4                       0.0          normal  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/raw/DataTrain_Preliminary.csv',sep = \";\",na_values = ['*',99999])\n",
    "df_test = pd.read_csv('../../data/raw/Data_Prediction.csv',sep = \";\")\n",
    "features = df.columns[0:-1]\n",
    "df = df.drop_duplicates(subset=features, keep=False)\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88044, 41), (22012, 41), (88044,), (22012,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## stratified shuffle\n",
    "X = df.drop(columns=\"type_of_attack\")\n",
    "\n",
    "#label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"type_of_attack\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make custom transformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def value_impute(data):\n",
    "    #if value is nan, then impute with 0, if value = inf, replcae with -1\n",
    "    if data == np.Inf:\n",
    "        data = -1\n",
    "    if data == np.nan:\n",
    "        data = 0\n",
    "    return data\n",
    "\n",
    "class FeatureEngineering(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_ = X.copy()\n",
    "        X_['src_bytes_per_duration'] = X_['src_bytes']/X_['duration']\n",
    "        X_['src_bytes_per_duration'] = X_.src_bytes_per_duration.apply(lambda x : value_impute(x))\n",
    "        \n",
    "        X_['dst_bytes_per_duration'] = X_['dst_bytes']/X_['duration']\n",
    "        X_['dst_bytes_per_duration'] = X_.dst_bytes_per_duration.apply(lambda x:value_impute(x))\n",
    "\n",
    "        X_['total_bytes_per_duration'] = (X_['src_bytes'] + X_[\"dst_bytes\"]) / X_['duration']\n",
    "        X_['total_bytes_per_duration'] = X_['total_bytes_per_duration'].apply(lambda x:value_impute(x))\n",
    "\n",
    "        X_['num_access_use'] = X_['num_root'] + X_['num_file_creations'] + X_['num_shells'] + X_['num_access_files'] + X_['num_outbound_cmds']\n",
    "        X_['num_access_use'] = X_.num_access_use.apply(lambda x:value_impute(x))\n",
    "\n",
    "        X_['total_bytes'] = X_['src_bytes'] + X_['dst_bytes']\n",
    "        X_['connection_rate'] = X_['count'] / X_['duration']\n",
    "        X_['connection_rate'] = X_['connection_rate'].apply(lambda x:value_impute(x))\n",
    "\n",
    "        X_['host_error_rate'] = X_[\"dst_host_serror_rate\"] + X_['dst_host_rerror_rate']\n",
    "        X_['success_login_ratio'] = X_['num_compromised'] / (X_['num_compromised'] + X_['num_failed_logins'])\n",
    "        X_['success_login_ratio'] = X_['success_login_ratio'].apply(lambda x:value_impute(x))\n",
    "\n",
    "        X_['connection_stability'] = X_[['serror_rate','rerror_rate','srv_serror_rate','srv_rerror_rate']].std(axis = 1)\n",
    "        X_['flag_rate']  = X_['count'] * (X_['serror_rate'] + X_['rerror_rate'] + X_['srv_serror_rate'] + X_['srv_rerror_rate'])\n",
    "\n",
    "        return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "#one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder,TargetEncoder\n",
    "\n",
    "cat_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "num_columns = X_train.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "feat_eng_cols = ['src_bytes_per_duration','dst_bytes_per_duration','total_bytes_per_duration','num_access_use','total_bytes',\n",
    "                 'connection_rate','host_error_rate','success_login_ratio','connection_stability','flag_rate']\n",
    "num_columns = num_columns + feat_eng_cols\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", PowerTransformer())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", TargetEncoder(target_type=\"continuous\",random_state=42))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "(\"numerical\",num_pipe,num_columns),\n",
    "(\"categorical\",cat_pipe,cat_columns)\n",
    "],remainder = \"drop\",verbose_feature_names_out=False).set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>num_access_use</th>\n",
       "      <th>total_bytes</th>\n",
       "      <th>connection_rate</th>\n",
       "      <th>host_error_rate</th>\n",
       "      <th>success_login_ratio</th>\n",
       "      <th>connection_stability</th>\n",
       "      <th>flag_rate</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37991</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>-1.134941</td>\n",
       "      <td>-0.920574</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>-0.814661</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>-1.130042</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>1.243288</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>1.236558</td>\n",
       "      <td>1.238821</td>\n",
       "      <td>3.207384</td>\n",
       "      <td>2.653904</td>\n",
       "      <td>3.146027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9261</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>1.263017</td>\n",
       "      <td>-0.920574</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>-0.814661</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>0.839250</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>1.147782</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.207384</td>\n",
       "      <td>3.617773</td>\n",
       "      <td>3.914765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27008</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>0.758474</td>\n",
       "      <td>0.910892</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>1.229286</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>0.640921</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>-0.903882</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.206033</td>\n",
       "      <td>3.852263</td>\n",
       "      <td>3.911250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49884</th>\n",
       "      <td>3.369467</td>\n",
       "      <td>0.617948</td>\n",
       "      <td>0.614867</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>-0.814661</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>0.407211</td>\n",
       "      <td>3.112262</td>\n",
       "      <td>-0.903882</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.938496</td>\n",
       "      <td>4.440353</td>\n",
       "      <td>3.914765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85361</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>-1.134941</td>\n",
       "      <td>-0.920574</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>-0.814661</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>-1.130042</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>0.399694</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>1.236558</td>\n",
       "      <td>1.352898</td>\n",
       "      <td>3.209313</td>\n",
       "      <td>2.576331</td>\n",
       "      <td>2.034315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17284</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>0.839891</td>\n",
       "      <td>0.754944</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>1.229286</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>0.583167</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>-0.903882</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.206033</td>\n",
       "      <td>3.852263</td>\n",
       "      <td>3.911250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79722</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>1.221190</td>\n",
       "      <td>0.876026</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>1.229286</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>0.862692</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>-0.903882</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.207384</td>\n",
       "      <td>3.939431</td>\n",
       "      <td>3.914765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40873</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>-0.044170</td>\n",
       "      <td>-0.920574</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>-0.814661</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>-0.285717</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>-0.903882</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.644563</td>\n",
       "      <td>1.831253</td>\n",
       "      <td>3.911250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47348</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>-1.134941</td>\n",
       "      <td>-0.920574</td>\n",
       "      <td>14.897002</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>-0.814661</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>-1.130042</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>1.243288</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>1.236558</td>\n",
       "      <td>1.351598</td>\n",
       "      <td>3.207442</td>\n",
       "      <td>2.660380</td>\n",
       "      <td>2.032085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21155</th>\n",
       "      <td>-0.307969</td>\n",
       "      <td>0.738663</td>\n",
       "      <td>1.524687</td>\n",
       "      <td>-0.060203</td>\n",
       "      <td>-0.116077</td>\n",
       "      <td>-0.057251</td>\n",
       "      <td>-0.160122</td>\n",
       "      <td>-0.075631</td>\n",
       "      <td>1.229286</td>\n",
       "      <td>-0.115092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156468</td>\n",
       "      <td>1.422714</td>\n",
       "      <td>-0.325724</td>\n",
       "      <td>-0.807862</td>\n",
       "      <td>-0.075465</td>\n",
       "      <td>-0.806005</td>\n",
       "      <td>-0.848228</td>\n",
       "      <td>3.207176</td>\n",
       "      <td>3.855611</td>\n",
       "      <td>3.919267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88044 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  src_bytes  dst_bytes       land  wrong_fragment    urgent  \\\n",
       "37991 -0.307969  -1.134941  -0.920574  -0.060203       -0.116077 -0.057251   \n",
       "9261  -0.307969   1.263017  -0.920574  -0.060203       -0.116077 -0.057251   \n",
       "27008 -0.307969   0.758474   0.910892  -0.060203       -0.116077 -0.057251   \n",
       "49884  3.369467   0.617948   0.614867  -0.060203       -0.116077 -0.057251   \n",
       "85361 -0.307969  -1.134941  -0.920574  -0.060203       -0.116077 -0.057251   \n",
       "...         ...        ...        ...        ...             ...       ...   \n",
       "17284 -0.307969   0.839891   0.754944  -0.060203       -0.116077 -0.057251   \n",
       "79722 -0.307969   1.221190   0.876026  -0.060203       -0.116077 -0.057251   \n",
       "40873 -0.307969  -0.044170  -0.920574  -0.060203       -0.116077 -0.057251   \n",
       "47348 -0.307969  -1.134941  -0.920574  14.897002       -0.116077 -0.057251   \n",
       "21155 -0.307969   0.738663   1.524687  -0.060203       -0.116077 -0.057251   \n",
       "\n",
       "            hot  num_failed_logins  logged_in  num_compromised  ...  \\\n",
       "37991 -0.160122          -0.075631  -0.814661        -0.115092  ...   \n",
       "9261  -0.160122          -0.075631  -0.814661        -0.115092  ...   \n",
       "27008 -0.160122          -0.075631   1.229286        -0.115092  ...   \n",
       "49884 -0.160122          -0.075631  -0.814661        -0.115092  ...   \n",
       "85361 -0.160122          -0.075631  -0.814661        -0.115092  ...   \n",
       "...         ...                ...        ...              ...  ...   \n",
       "17284 -0.160122          -0.075631   1.229286        -0.115092  ...   \n",
       "79722 -0.160122          -0.075631   1.229286        -0.115092  ...   \n",
       "40873 -0.160122          -0.075631  -0.814661        -0.115092  ...   \n",
       "47348 -0.160122          -0.075631  -0.814661        -0.115092  ...   \n",
       "21155 -0.160122          -0.075631   1.229286        -0.115092  ...   \n",
       "\n",
       "       num_access_use  total_bytes  connection_rate  host_error_rate  \\\n",
       "37991       -0.156468    -1.130042        -0.325724         1.243288   \n",
       "9261        -0.156468     0.839250        -0.325724         1.147782   \n",
       "27008       -0.156468     0.640921        -0.325724        -0.903882   \n",
       "49884       -0.156468     0.407211         3.112262        -0.903882   \n",
       "85361       -0.156468    -1.130042        -0.325724         0.399694   \n",
       "...               ...          ...              ...              ...   \n",
       "17284       -0.156468     0.583167        -0.325724        -0.903882   \n",
       "79722       -0.156468     0.862692        -0.325724        -0.903882   \n",
       "40873       -0.156468    -0.285717        -0.325724        -0.903882   \n",
       "47348       -0.156468    -1.130042        -0.325724         1.243288   \n",
       "21155       -0.156468     1.422714        -0.325724        -0.807862   \n",
       "\n",
       "       success_login_ratio  connection_stability  flag_rate  protocol_type  \\\n",
       "37991            -0.075465              1.236558   1.238821       3.207384   \n",
       "9261             -0.075465             -0.806005  -0.848228       3.207384   \n",
       "27008            -0.075465             -0.806005  -0.848228       3.206033   \n",
       "49884            -0.075465             -0.806005  -0.848228       3.938496   \n",
       "85361            -0.075465              1.236558   1.352898       3.209313   \n",
       "...                    ...                   ...        ...            ...   \n",
       "17284            -0.075465             -0.806005  -0.848228       3.206033   \n",
       "79722            -0.075465             -0.806005  -0.848228       3.207384   \n",
       "40873            -0.075465             -0.806005  -0.848228       3.644563   \n",
       "47348            -0.075465              1.236558   1.351598       3.207442   \n",
       "21155            -0.075465             -0.806005  -0.848228       3.207176   \n",
       "\n",
       "        service      flag  \n",
       "37991  2.653904  3.146027  \n",
       "9261   3.617773  3.914765  \n",
       "27008  3.852263  3.911250  \n",
       "49884  4.440353  3.914765  \n",
       "85361  2.576331  2.034315  \n",
       "...         ...       ...  \n",
       "17284  3.852263  3.911250  \n",
       "79722  3.939431  3.914765  \n",
       "40873  1.831253  3.911250  \n",
       "47348  2.660380  2.032085  \n",
       "21155  3.855611  3.919267  \n",
       "\n",
       "[88044 rows x 51 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_pipeline = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocessor\",preprocessor)\n",
    "])\n",
    "prep_pipeline.fit_transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lgbm selesai di training\n",
      "score test 0.973120902126283\n",
      "score cv 0.9751522738805788\n",
      "score train 0.9900993411339969\n",
      "acc score test 0.9962747592222424\n",
      "acc score cv 0.9965244651674212\n",
      "=====================================\n",
      "model xgb selesai di training\n",
      "score test 0.9727206176237437\n",
      "score cv 0.975943671093231\n",
      "score train 0.9862228376060374\n",
      "acc score test 0.9962747592222424\n",
      "acc score cv 0.9966380447413293\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"model\",None)\n",
    "])\n",
    "\n",
    "list_model = {\n",
    "    \"lgbm\":LGBMClassifier(verbose=-1,random_state=42),\n",
    "    \"xgb\":XGBClassifier(verbosity=0,random_state=42,tree_method=\"gpu_hist\",gpu_id=1),\n",
    "   # \"catB\" : CatBoostClassifier(verbose=0,n_estimators=200,random_seed=42,devices=\"gpu\"),\n",
    "}\n",
    "\n",
    "from sklearn.metrics import f1_score,make_scorer\n",
    "f1_scorer = make_scorer(f1_score,average = \"macro\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "df_model = train_model(list_model,X_train,y_train,X_test,y_test,f1_score,cv,f1_scorer,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cpy = X_test.copy()\n",
    "X_test_cpy['predict'] = df_model.model[0].predict(X_test)\n",
    "X_test_cpy['actual'] = y_test\n",
    "X_test_cpy['predict'] = le.inverse_transform(X_test_cpy['predict'])\n",
    "X_test_cpy['actual'] = le.inverse_transform(X_test_cpy['actual'])\n",
    "\n",
    "#show the observation that is wrongly classified\n",
    "X_test_cpy[X_test_cpy['predict'] != X_test_cpy['actual']].to_csv(\"../../data/interim/wrongly_classified.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model lgbm selesai di training\n",
      "score test 0.973120902126283\n",
      "score cv 0.9751522738805788\n",
      "score train 0.9900993411339969\n",
      "=====================================\n",
      "model xgb selesai di training\n",
      "score test 0.9727206176237437\n",
      "score cv 0.975943671093231\n",
      "score train 0.9862228376060374\n",
      "=====================================\n",
      "model catB selesai di training\n",
      "score test 0.9724801846295884\n",
      "score cv 0.9729820231608584\n",
      "score train 0.9807517926110468\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"model\",None)\n",
    "])\n",
    "\n",
    "list_model = {\n",
    "    \"lgbm\":LGBMClassifier(verbose=-1,random_state=42),\n",
    "    \"xgb\":XGBClassifier(verbosity=0,random_state=42,tree_method=\"gpu_hist\",gpu_id=1),\n",
    "    \"catB\" : CatBoostClassifier(verbose=0,n_estimators=200,random_seed=42,devices=\"gpu\"),\n",
    "}\n",
    "\n",
    "from sklearn.metrics import f1_score,make_scorer\n",
    "f1_scorer = make_scorer(f1_score,average = \"macro\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "df_model = train_model(list_model,X_train,y_train,X_test,y_test,f1_score,cv,f1_scorer,pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "pipeline_rfe_lgb = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"model\",LGBMClassifier(verbose=-1,random_state=42))\n",
    "])\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "rfe_lgb = rfecv(X_train,y_train,pipeline_rfe_lgb,scoring_metric=f1_scorer,scoring_decimals=8,cv=cv,step=2,min_features_to_select=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_file_creations', 'num_shells', 'is_host_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n"
     ]
    }
   ],
   "source": [
    "print(rfe_lgb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def tuning_model(objective,pipeline,n_trial = 100):\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study.optimize(objective, n_trials=n_trial,show_progress_bar=True,)\n",
    "    best_params = study.best_params\n",
    "    pipeline.set_params(**best_params)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    test_score = f1_score(y_test, pipeline.predict(X_test),average = \"macro\")\n",
    "    train_score = f1_score(y_train, pipeline.predict(X_train),average = \"macro\")\n",
    "    valid_score = study.best_value\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"best params:\", best_params)\n",
    "    print(\"train score:\", train_score)\n",
    "    print(\"test score:\", test_score)\n",
    "    print(\"valid score:\", valid_score)\n",
    "    print(classification_report(y_test, y_pred,digits=5))\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.975984:  13%|█▎        | 4/30 [02:13<14:25, 33.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2023-10-01 20:33:47,603] Trial 4 failed with parameters: {'algo__colsample_bytree': 0.16924599954905534, 'algo__subsample': 0.10846644831147675, 'algo__reg_alpha': 1.55101676733483e-05, 'algo__reg_lambda': 0.019600659336465508, 'algo__max_depth': 8, 'algo__max_bin': 421} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_12900\\2323638350.py\", line 39, in objective\n",
      "    return cross_val_score(pipeline_rfe_lgb_tune, X_train, y_train, cv=cv, scoring=f1_scorer).mean()\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 211, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py\", line 423, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py\", line 957, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 754, in fit_transform\n",
      "    result = self._fit_transform(X, y, _fit_transform_one)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 681, in _fit_transform\n",
      "    return Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py\", line 957, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py\", line 479, in fit_transform\n",
      "    return last_step.fit_transform(Xt, y, **fit_params_last_step)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3142, in fit_transform\n",
      "    return self._fit(X, y, force_transform=True)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3174, in _fit\n",
      "    self.lambdas_[i] = optim_function(col)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3363, in _yeo_johnson_optimize\n",
      "    return optimize.brent(_neg_log_likelihood, brack=(-2, 2))\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 2642, in brent\n",
      "    res = _minimize_scalar_brent(func, brack, args, **options)\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 2679, in _minimize_scalar_brent\n",
      "    brent.optimize()\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\scipy\\optimize\\_optimize.py\", line 2521, in optimize\n",
      "    fu = func(*((u,) + self.args))      # calculate new output value\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 3355, in _neg_log_likelihood\n",
      "    loglike += (lmbda - 1) * (np.sign(x) * np.log1p(np.abs(x))).sum()\n",
      "  File \"d:\\conda\\envs\\lomba-dac\\lib\\site-packages\\numpy\\core\\_methods.py\", line 49, in _sum\n",
      "    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "KeyboardInterrupt\n",
      "[W 2023-10-01 20:33:47,683] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\study\\dac\\notebooks\\eda\\feature_engineering.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     pipeline_rfe_lgb_tune\u001b[39m.\u001b[39mset_params(algo\u001b[39m=\u001b[39mmodel)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cross_val_score(pipeline_rfe_lgb_tune, X_train, y_train, cv\u001b[39m=\u001b[39mcv, scoring\u001b[39m=\u001b[39mf1_scorer)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m pipeline_lgbm \u001b[39m=\u001b[39m tuning_model(objective,pipeline_rfe_lgb_tune,n_trial \u001b[39m=\u001b[39;49m \u001b[39m30\u001b[39;49m)\n",
      "\u001b[1;32me:\\study\\dac\\notebooks\\eda\\feature_engineering.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optuna\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mset_verbosity(optuna\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mWARNING)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49mn_trial,show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pipeline\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbest_params)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\optuna\\study\\study.py:442\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     _optimize(\n\u001b[0;32m    443\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    444\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    445\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    446\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    447\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    448\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    449\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    450\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    451\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    452\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32me:\\study\\dac\\notebooks\\eda\\feature_engineering.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m model \u001b[39m=\u001b[39m LGBMClassifier(max_depth\u001b[39m=\u001b[39mmax_depth, \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                       \u001b[39m#  n_estimators=n_estimators,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                       \u001b[39m# learning_rate=learning_rate, \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                        n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                        verbose \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m pipeline_rfe_lgb_tune\u001b[39m.\u001b[39mset_params(algo\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/study/dac/notebooks/eda/feature_engineering.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cross_val_score(pipeline_rfe_lgb_tune, X_train, y_train, cv\u001b[39m=\u001b[39;49mcv, scoring\u001b[39m=\u001b[39;49mf1_scorer)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    574\u001b[0m )\n\u001b[0;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m         clone(estimator),\n\u001b[0;32m    312\u001b[0m         X,\n\u001b[0;32m    313\u001b[0m         y,\n\u001b[0;32m    314\u001b[0m         scorers,\n\u001b[0;32m    315\u001b[0m         train,\n\u001b[0;32m    316\u001b[0m         test,\n\u001b[0;32m    317\u001b[0m         verbose,\n\u001b[0;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    319\u001b[0m         fit_params,\n\u001b[0;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    324\u001b[0m     )\n\u001b[0;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[0;32m    326\u001b[0m )\n\u001b[0;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py:423\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    398\u001b[0m \n\u001b[0;32m    399\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    422\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 423\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    424\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py:377\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    375\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    376\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    378\u001b[0m     cloned_transformer,\n\u001b[0;32m    379\u001b[0m     X,\n\u001b[0;32m    380\u001b[0m     y,\n\u001b[0;32m    381\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    382\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    383\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    384\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    385\u001b[0m )\n\u001b[0;32m    386\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:754\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[0;32m    752\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n\u001b[1;32m--> 754\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_transform(X, y, _fit_transform_one)\n\u001b[0;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result:\n\u001b[0;32m    757\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fitted_transformers([])\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:681\u001b[0m, in \u001b[0;36mColumnTransformer._fit_transform\u001b[1;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[0;32m    675\u001b[0m transformers \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m    676\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter(\n\u001b[0;32m    677\u001b[0m         fitted\u001b[39m=\u001b[39mfitted, replace_strings\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, column_as_strings\u001b[39m=\u001b[39mcolumn_as_strings\n\u001b[0;32m    678\u001b[0m     )\n\u001b[0;32m    679\u001b[0m )\n\u001b[0;32m    680\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[0;32m    682\u001b[0m         delayed(func)(\n\u001b[0;32m    683\u001b[0m             transformer\u001b[39m=\u001b[39;49mclone(trans) \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m fitted \u001b[39melse\u001b[39;49;00m trans,\n\u001b[0;32m    684\u001b[0m             X\u001b[39m=\u001b[39;49m_safe_indexing(X, column, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),\n\u001b[0;32m    685\u001b[0m             y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    686\u001b[0m             weight\u001b[39m=\u001b[39;49mweight,\n\u001b[0;32m    687\u001b[0m             message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mColumnTransformer\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    688\u001b[0m             message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(name, idx, \u001b[39mlen\u001b[39;49m(transformers)),\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m         \u001b[39mfor\u001b[39;49;00m idx, (name, trans, column, weight) \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(transformers, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    691\u001b[0m     )\n\u001b[0;32m    692\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(e):\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py:957\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 957\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    958\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\pipeline.py:479\u001b[0m, in \u001b[0;36mPipeline.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    477\u001b[0m fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(last_step, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 479\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit_transform(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m     \u001b[39mreturn\u001b[39;00m last_step\u001b[39m.\u001b[39mfit(Xt, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_last_step)\u001b[39m.\u001b[39mtransform(Xt)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3142\u001b[0m, in \u001b[0;36mPowerTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   3124\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   3125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   3126\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit `PowerTransformer` to `X`, then transform `X`.\u001b[39;00m\n\u001b[0;32m   3127\u001b[0m \n\u001b[0;32m   3128\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3140\u001b[0m \u001b[39m        Transformed data.\u001b[39;00m\n\u001b[0;32m   3141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, force_transform\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3174\u001b[0m, in \u001b[0;36mPowerTransformer._fit\u001b[1;34m(self, X, y, force_transform)\u001b[0m\n\u001b[0;32m   3171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambdas_[i] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m   3172\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m-> 3174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambdas_[i] \u001b[39m=\u001b[39m optim_function(col)\n\u001b[0;32m   3176\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstandardize \u001b[39mor\u001b[39;00m force_transform:\n\u001b[0;32m   3177\u001b[0m     X[:, i] \u001b[39m=\u001b[39m transform_function(X[:, i], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlambdas_[i])\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3363\u001b[0m, in \u001b[0;36mPowerTransformer._yeo_johnson_optimize\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   3361\u001b[0m x \u001b[39m=\u001b[39m x[\u001b[39m~\u001b[39mnp\u001b[39m.\u001b[39misnan(x)]\n\u001b[0;32m   3362\u001b[0m \u001b[39m# choosing bracket -2, 2 like for boxcox\u001b[39;00m\n\u001b[1;32m-> 3363\u001b[0m \u001b[39mreturn\u001b[39;00m optimize\u001b[39m.\u001b[39;49mbrent(_neg_log_likelihood, brack\u001b[39m=\u001b[39;49m(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2642\u001b[0m, in \u001b[0;36mbrent\u001b[1;34m(func, args, brack, tol, full_output, maxiter)\u001b[0m\n\u001b[0;32m   2570\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2571\u001b[0m \u001b[39mGiven a function of one variable and a possible bracket, return\u001b[39;00m\n\u001b[0;32m   2572\u001b[0m \u001b[39ma local minimizer of the function isolated to a fractional precision\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2638\u001b[0m \n\u001b[0;32m   2639\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m options \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mxtol\u001b[39m\u001b[39m'\u001b[39m: tol,\n\u001b[0;32m   2641\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m: maxiter}\n\u001b[1;32m-> 2642\u001b[0m res \u001b[39m=\u001b[39m _minimize_scalar_brent(func, brack, args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m   2643\u001b[0m \u001b[39mif\u001b[39;00m full_output:\n\u001b[0;32m   2644\u001b[0m     \u001b[39mreturn\u001b[39;00m res[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m], res[\u001b[39m'\u001b[39m\u001b[39mnfev\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2679\u001b[0m, in \u001b[0;36m_minimize_scalar_brent\u001b[1;34m(func, brack, args, xtol, maxiter, disp, **unknown_options)\u001b[0m\n\u001b[0;32m   2676\u001b[0m brent \u001b[39m=\u001b[39m Brent(func\u001b[39m=\u001b[39mfunc, args\u001b[39m=\u001b[39margs, tol\u001b[39m=\u001b[39mtol,\n\u001b[0;32m   2677\u001b[0m               full_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, maxiter\u001b[39m=\u001b[39mmaxiter, disp\u001b[39m=\u001b[39mdisp)\n\u001b[0;32m   2678\u001b[0m brent\u001b[39m.\u001b[39mset_bracket(brack)\n\u001b[1;32m-> 2679\u001b[0m brent\u001b[39m.\u001b[39;49moptimize()\n\u001b[0;32m   2680\u001b[0m x, fval, nit, nfev \u001b[39m=\u001b[39m brent\u001b[39m.\u001b[39mget_result(full_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   2682\u001b[0m success \u001b[39m=\u001b[39m nit \u001b[39m<\u001b[39m maxiter \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (np\u001b[39m.\u001b[39misnan(x) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39misnan(fval))\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\scipy\\optimize\\_optimize.py:2521\u001b[0m, in \u001b[0;36mBrent.optimize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2520\u001b[0m     u \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m rat\n\u001b[1;32m-> 2521\u001b[0m fu \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49m((u,) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs))      \u001b[39m# calculate new output value\u001b[39;00m\n\u001b[0;32m   2522\u001b[0m funcalls \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2524\u001b[0m \u001b[39mif\u001b[39;00m (fu \u001b[39m>\u001b[39m fx):                 \u001b[39m# if it's bigger than current\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:3355\u001b[0m, in \u001b[0;36mPowerTransformer._yeo_johnson_optimize.<locals>._neg_log_likelihood\u001b[1;34m(lmbda)\u001b[0m\n\u001b[0;32m   3353\u001b[0m log_var \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlog(x_trans_var)\n\u001b[0;32m   3354\u001b[0m loglike \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mn_samples \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m log_var\n\u001b[1;32m-> 3355\u001b[0m loglike \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (lmbda \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (np\u001b[39m.\u001b[39;49msign(x) \u001b[39m*\u001b[39;49m np\u001b[39m.\u001b[39;49mlog1p(np\u001b[39m.\u001b[39;49mabs(x)))\u001b[39m.\u001b[39;49msum()\n\u001b[0;32m   3357\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mloglike\n",
      "File \u001b[1;32md:\\conda\\envs\\lomba-dac\\lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sum\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[39m=\u001b[39m_NoValue, where\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_cols_lgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', \n",
    "                 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', \n",
    "                 'num_root', 'num_file_creations', 'num_shells', 'is_guest_login', 'count', 'srv_count', 'serror_rate', \n",
    "                 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
    "                 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "                 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "drop_cols_lgb = [col for col in X_train.columns if col not in best_cols_lgb]\n",
    "\n",
    "pipeline_rfe_lgb_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"drop_cols\",FunctionTransformer(lambda x: x.drop(drop_cols_lgb,axis = 1))),\n",
    "    (\"algo\",LGBMClassifier(verbose=-1,random_state=42))\n",
    "])\n",
    "\n",
    "def objective(trial):\n",
    "    #n_estimators = trial.suggest_int(\"algo__n_estimators\", 100, 500, step=100)\n",
    "    col_sample_bytree = trial.suggest_float(\"algo__colsample_bytree\", 0.1, 1.0)\n",
    "    subsample = trial.suggest_float(\"algo__subsample\", 0.1, 1.0)\n",
    "    reg_alpha = trial.suggest_float(\"algo__reg_alpha\", 1e-5, 1e-1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"algo__reg_lambda\", 1e-5, 1e-1, log=True)\n",
    "    max_depth = trial.suggest_int(\"algo__max_depth\", 1, 10)\n",
    "    max_bin = trial.suggest_int(\"algo__max_bin\", 200, 1000)\n",
    "    \n",
    "    #learning_rate = trial.suggest_float(\"algo__learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    model = LGBMClassifier(max_depth=max_depth, \n",
    "                          #  n_estimators=n_estimators,\n",
    "                          # learning_rate=learning_rate, \n",
    "                           colsample_bytree=col_sample_bytree,\n",
    "                           subsample=subsample,\n",
    "                           reg_alpha=reg_alpha,\n",
    "                           reg_lambda=reg_lambda,\n",
    "                           max_bin=max_bin,\n",
    "                           random_state=42, \n",
    "                           n_jobs=-1,\n",
    "                           verbose = -1,)\n",
    "    pipeline_rfe_lgb_tune.set_params(algo=model)\n",
    "    return cross_val_score(pipeline_rfe_lgb_tune, X_train, y_train, cv=cv, scoring=f1_scorer).mean()\n",
    "\n",
    "pipeline_lgbm = tuning_model(objective,pipeline_rfe_lgb_tune,n_trial = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score,make_scorer\n",
    "f1_scorer = make_scorer(f1_score,average = \"macro\")\n",
    "\n",
    "\n",
    "pipeline_rfe_xgb = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"model\",XGBClassifier(verbosity=0,random_state=42,tree_method=\"gpu_hist\"))\n",
    "])\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=False)\n",
    "rfe_xgb = rfecv(X_train,y_train,pipeline_rfe_xgb,scoring_metric=f1_scorer,scoring_decimals=8,cv=cv,step=2,min_features_to_select=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97634782\n",
      "['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_shells', 'is_host_login', 'count', 'srv_count', 'serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n"
     ]
    }
   ],
   "source": [
    "print(rfe_xgb[1])\n",
    "print(rfe_xgb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 23. Best value: 0.975114: 100%|██████████| 30/30 [11:31<00:00, 23.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'algo__max_depth': 4, 'algo__reg_alpha': 0.00040717338276114926, 'algo__reg_lambda': 0.000975237644841343, 'algo__colsample_bytree': 0.825606327123948, 'algo__gamma': 6.618221857652686e-05}\n",
      "train score: 0.984265106631578\n",
      "test score: 0.9737641424824068\n",
      "valid score: 0.9751138238832082\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99701   0.99107   0.99403       336\n",
      "           1    0.91653   0.97340   0.94411       564\n",
      "           2    0.99959   1.00000   0.99979      7304\n",
      "           3    0.92627   0.79762   0.85714       252\n",
      "           4    0.99950   0.99967   0.99958     11998\n",
      "           5    1.00000   0.99809   0.99904       523\n",
      "           6    0.99820   0.99462   0.99641       558\n",
      "           7    1.00000   1.00000   1.00000       472\n",
      "\n",
      "    accuracy                        0.99650     22007\n",
      "   macro avg    0.97964   0.96931   0.97376     22007\n",
      "weighted avg    0.99652   0.99650   0.99643     22007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## XGBoost\n",
    "import optuna\n",
    "\n",
    "best_cols_xgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'hot',\n",
    "                  'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_shells', 'is_host_login', 'count', \n",
    "                  'srv_count', 'serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
    "                  'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', \n",
    "                  'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "best_cols_xgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "drop_cols_xgb = [col for col in X_train.columns if col not in best_cols_xgb]\n",
    "\n",
    "pipeline_rfe_xgb_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    ('prep', preprocessor),\n",
    "    (\"drop_cols\",FunctionTransformer(lambda x: x.drop(drop_cols_xgb,axis = 1))),\n",
    "    ('algo', XGBClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "def objective(trial):\n",
    "   # n_estimators = trial.suggest_int(\"algo__n_estimators\", 100, 1000, step=100)\n",
    "    max_depth = trial.suggest_int(\"algo__max_depth\", 1, 10)\n",
    "    reg_alpha = trial.suggest_float(\"algo__reg_alpha\", 1e-5, 1e-1, log=True)\n",
    "    reg_lambda = trial.suggest_float(\"algo__reg_lambda\", 1e-5, 1e-1, log=True)\n",
    "    colsample_bytree = trial.suggest_float(\"algo__colsample_bytree\", 0.1, 1.0)\n",
    "    gamma = trial.suggest_float(\"algo__gamma\", 1e-5, 1e-1, log=True)\n",
    "    model = XGBClassifier(max_depth=max_depth, \n",
    "                          gamma=gamma,\n",
    "                          reg_alpha=reg_alpha,\n",
    "                          reg_lambda=reg_lambda,\n",
    "                          colsample_bytree=colsample_bytree,\n",
    "                           random_state=42, n_jobs=-1,\n",
    "                           tree_method = \"gpu_hist\",\n",
    "                           )\n",
    "    pipeline_rfe_xgb_tune.set_params(algo=model)\n",
    "    return cross_val_score(pipeline_rfe_xgb_tune, X_train, y_train, cv=cv, scoring=f1_scorer).mean()\n",
    "\n",
    "pipeline_xgb = tuning_model(objective,pipeline_rfe_xgb_tune,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best trial: 22. Best value: 0.977029: 100%|██████████| 30/30 [17:26<00:00, 34.90s/it]\n",
    "best params: {'algo__max_depth': 10, 'algo__reg_alpha': 0.006570999279081406, 'algo__reg_lambda': 0.0015967627181379426, 'algo__colsample_bytree': 0.9012229217837533, 'algo__gamma': 0.00541634111409239}\n",
    "train score: 0.9915338170347747\n",
    "test score: 0.9712022221252201\n",
    "valid score: 0.9770285516898067\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0    0.99107   0.98813   0.98960       337\n",
    "           1    0.91837   0.95745   0.93750       564\n",
    "           2    0.99973   0.99973   0.99973      7305\n",
    "           3    0.89778   0.80159   0.84696       252\n",
    "           4    0.99925   0.99958   0.99942     12001\n",
    "           5    1.00000   1.00000   1.00000       523\n",
    "           6    0.99642   0.99642   0.99642       558\n",
    "           7    1.00000   1.00000   1.00000       472\n",
    "\n",
    "    accuracy                        0.99605     22012\n",
    "   macro avg    0.97533   0.96786   0.97120     22012\n",
    "weighted avg    0.99601   0.99605   0.99599     22012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967175508300876"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cols_lgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_root', 'num_file_creations', 'num_shells', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "drop_cols_lgb = [col for col in X_train.columns if col not in best_cols_lgb]\n",
    "best_params_lgb = {'algo__colsample_bytree': 0.7511822907836249, 'algo__subsample': 0.3149295796873115, 'algo__reg_alpha': 0.00035731264003581947, 'algo__reg_lambda': 0.0034764373812413854, 'algo__max_depth': 8, 'algo__max_bin': 377}\n",
    "\n",
    "pipeline_rfe_lgb_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"drop_cols\",FunctionTransformer(lambda x: x.drop(drop_cols_lgb,axis = 1))),\n",
    "    (\"algo\",LGBMClassifier(verbose=-1,random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_rfe_lgb_tune.set_params(**best_params_lgb)\n",
    "\n",
    "cross_val_score(pipeline_rfe_lgb_tune, X_train, y_train, cv=cv, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99405   0.99110   0.99257       337\n",
      "           1    0.91511   0.95567   0.93495       564\n",
      "           2    0.99986   0.99973   0.99979      7305\n",
      "           3    0.89238   0.78968   0.83789       252\n",
      "           4    0.99900   0.99983   0.99942     12001\n",
      "           5    0.99809   0.99809   0.99809       523\n",
      "           6    1.00000   0.99283   0.99640       558\n",
      "           7    1.00000   1.00000   1.00000       472\n",
      "\n",
      "    accuracy                        0.99591     22012\n",
      "   macro avg    0.97481   0.96587   0.96989     22012\n",
      "weighted avg    0.99587   0.99591   0.99584     22012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pipeline_rfe_lgb_tune.fit(X_train,y_train)\n",
    "print(classification_report(y_test,pipeline_rfe_lgb_tune.predict(X_test),digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967629827886585"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cols_xgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_shells', 'is_host_login', 'count', 'srv_count', 'serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "drop_cols_xgb = [col for col in X_train.columns if col not in best_cols_xgb]\n",
    "best_params_xgb = {'algo__max_depth': 10, 'algo__reg_alpha': 0.006570999279081406, 'algo__reg_lambda': 0.0015967627181379426, 'algo__colsample_bytree': 0.9012229217837533, 'algo__gamma': 0.00541634111409239}\n",
    "\n",
    "pipeline_rfe_xgb_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    ('prep', preprocessor),\n",
    "    (\"drop_cols\",FunctionTransformer(lambda x: x.drop(drop_cols_xgb,axis = 1))),\n",
    "    ('algo', XGBClassifier(random_state=42, n_jobs=-1,tree_method = \"gpu_hist\"))\n",
    "])\n",
    "\n",
    "pipeline_rfe_xgb_tune.set_params(**best_params_xgb)\n",
    "cross_val_score(pipeline_rfe_xgb_tune, X_train, y_train, cv=cv, scoring=\"accuracy\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99107   0.98813   0.98960       337\n",
      "           1    0.91837   0.95745   0.93750       564\n",
      "           2    0.99973   0.99973   0.99973      7305\n",
      "           3    0.89778   0.80159   0.84696       252\n",
      "           4    0.99925   0.99958   0.99942     12001\n",
      "           5    1.00000   1.00000   1.00000       523\n",
      "           6    0.99642   0.99642   0.99642       558\n",
      "           7    1.00000   1.00000   1.00000       472\n",
      "\n",
      "    accuracy                        0.99605     22012\n",
      "   macro avg    0.97533   0.96786   0.97120     22012\n",
      "weighted avg    0.99601   0.99605   0.99599     22012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_rfe_xgb_tune.fit(X_train,y_train)\n",
    "print(classification_report(y_test,pipeline_rfe_xgb_tune.predict(X_test),digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier,VotingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train_cat = X_train.select_dtypes(include=['object'])\n",
    "X_train_num = X_train.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", PowerTransformer())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", TargetEncoder(target_type=\"continuous\"))\n",
    "])\n",
    "\n",
    "preprocessor_base = ColumnTransformer([\n",
    "(\"numerical\",num_pipe,X_train_num.columns),\n",
    "(\"categorical\",cat_pipe,X_train_cat.columns)\n",
    "],remainder = \"drop\",verbose_feature_names_out=False)\n",
    "\n",
    "pipeline_lgb_base = Pipeline([\n",
    "    (\"preprocess\",preprocessor_base),\n",
    "    (\"algo\",LGBMClassifier(verbose=-1,random_state=42))\n",
    "])\n",
    "pipeline_xgb_base = Pipeline([\n",
    "     (\"preprocess\",preprocessor_base),\n",
    "     (\"algo\",XGBClassifier(random_state=42, n_jobs=-1,tree_method = \"gpu_hist\"))\n",
    "    ])\n",
    "\n",
    "pipeline_catb_base = Pipeline([\n",
    "    (\"preprocess\",preprocessor_base),\n",
    "    (\"algo\",CatBoostClassifier(verbose=0,n_estimators=300,random_seed=42,devices=\"gpu\"))\n",
    "])\n",
    "\n",
    "best_cols_lgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_root', 'num_file_creations', 'num_shells', 'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "drop_cols_lgb = [col for col in X_train.columns if col not in best_cols_lgb]\n",
    "best_params_lgb = {'algo__colsample_bytree': 0.7511822907836249, 'algo__subsample': 0.3149295796873115, 'algo__reg_alpha': 0.00035731264003581947, 'algo__reg_lambda': 0.0034764373812413854, 'algo__max_depth': 8, 'algo__max_bin': 377}\n",
    "\n",
    "pipeline_rfe_lgb_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    (\"preprocess\",preprocessor),\n",
    "    (\"drop_cols\",FunctionTransformer(lambda x: x.drop(drop_cols_lgb,axis = 1))),\n",
    "    (\"algo\",LGBMClassifier(verbose=-1,random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_rfe_lgb_tune.set_params(**best_params_lgb)\n",
    "\n",
    "## XGBoost\n",
    "import optuna\n",
    "\n",
    "best_cols_xgb = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'num_shells', 'is_host_login', 'count', 'srv_count', 'serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "drop_cols_xgb = [col for col in X_train.columns if col not in best_cols_xgb]\n",
    "best_params_xgb = {'algo__max_depth': 10, 'algo__reg_alpha': 0.006570999279081406, 'algo__reg_lambda': 0.0015967627181379426, 'algo__colsample_bytree': 0.9012229217837533, 'algo__gamma': 0.00541634111409239}\n",
    "\n",
    "pipeline_rfe_xgb_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    ('prep', preprocessor),\n",
    "    (\"drop_cols\",FunctionTransformer(lambda x: x.drop(drop_cols_xgb,axis = 1))),\n",
    "    ('algo', XGBClassifier(random_state=42, n_jobs=-1,tree_method = \"gpu_hist\"))\n",
    "])\n",
    "\n",
    "pipeline_rfe_xgb_tune.set_params(**best_params_xgb)\n",
    "\n",
    "best_params_catb = {'algo__max_depth': 8, 'algo__colsample_bylevel': 0.6986321466355678}\n",
    "pipeline_catb_rfe_tune = Pipeline([\n",
    "    (\"feature_engineering\",FeatureEngineering()),\n",
    "    ('prep', preprocessor),\n",
    "    (\"algo\",CatBoostClassifier(verbose=0,n_estimators=100,random_seed=42,devices=\"gpu\"))\n",
    "])\n",
    "\n",
    "pipeline_catb_rfe_tune.set_params(**best_params_catb)\n",
    "\n",
    "estimators = [\n",
    "    (\"lgbm\",pipeline_lgb_base),\n",
    "    (\"xgb\",pipeline_xgb_base),\n",
    "    #(\"catb\",pipeline_catb_base),\n",
    "    (\"lgbm_tune\",pipeline_rfe_lgb_tune),\n",
    "    (\"xgb_tune\",pipeline_rfe_xgb_tune),\n",
    "   # (\"catb_tune\",pipeline_catb_rfe_tune)\n",
    "]\n",
    "\n",
    "pipeline_stack = VotingClassifier(estimators=estimators,voting = 'soft',verbose=1,)\n",
    "\n",
    "cv_score = cross_val_score(pipeline_stack,X_train,y_train,cv = cv,scoring = \"accuracy\",n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967970556287702"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Voting] ..................... (1 of 4) Processing lgbm, total=   3.8s\n",
      "[Voting] ...................... (2 of 4) Processing xgb, total=   4.8s\n",
      "[Voting] ................ (3 of 4) Processing lgbm_tune, total=   5.4s\n",
      "[Voting] ................. (4 of 4) Processing xgb_tune, total=   6.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9740759781156713"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_stack.fit(X_train,y_train)\n",
    "f1_score(y_test,pipeline_stack.predict(X_test),average = \"macro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.99107   0.98813   0.98960       337\n",
      "           1    0.92256   0.97163   0.94646       564\n",
      "           2    0.99973   0.99973   0.99973      7305\n",
      "           3    0.92694   0.80556   0.86200       252\n",
      "           4    0.99908   0.99967   0.99938     12001\n",
      "           5    1.00000   0.99809   0.99904       523\n",
      "           6    0.99820   0.99462   0.99641       558\n",
      "           7    1.00000   1.00000   1.00000       472\n",
      "\n",
      "    accuracy                        0.99641     22012\n",
      "   macro avg    0.97970   0.96968   0.97408     22012\n",
      "weighted avg    0.99641   0.99641   0.99634     22012\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,pipeline_stack.predict(X_test),digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['type_of_attack'] = le.inverse_transform(pipeline_stack.predict(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(\"../../data/processed/submission.csv\",index = False,sep = \";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lomba-dac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
